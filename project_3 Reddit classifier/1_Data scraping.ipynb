{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a subreddit classifier\n",
    "An exploration of nlp packages and evaluation of classifier models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement\n",
    "The world of investing is a multifaceted universe. Long gone are the days of simple shareholding. Today, traders are able to trade a variety of securities that suit their needs best. We want to zoom in on 2 major classes of securities - stocks and options, which are a derivative of stocks. There are traders out there who exclusively trade stocks, and others who exclusively trade options. Both are still able to make money. In this project, we will seek to understand posts from 2 subreddits devoted to these 2 classes of traders - r/stocks and r/options. \n",
    "\n",
    "**Problem statement:** What are the most representative text features of posts from each subreddit that will allow us to correctly classify them?\n",
    "\n",
    "To answer this question, we will explore a variety of classification models and natural language processing (nlp) tools to build a classifier that will reliably be able to determine if a given post is from r/options or r/stocks. This classifier should be able to identify for us the top predictive features for each subreddit, and from there, we hope to additionally answer questions such as \"what trading strategies are currently trending within option and stock traders\" and \"what companies are currently popular with traders\". This information will be useful to traders who want to better understand current trends within the market, regardless of whether he trades more in options or in stocks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Executive Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project seeks to build a classifier that is able to differentiate between posts from r/options and r/stocks, two trading subreddits focused on different classes of securities. The classifier should be able to identify what features are important in discriminating between these subreddits and use them to make accurate predictions.\n",
    "\n",
    "The methodology of our project is as follows:\n",
    "\n",
    "    1. Scrape data from our 2 subreddits\n",
    "        - we scrape by top posts, by month. Data was scraped on 10th March 2021.\n",
    "    2. Data Cleaning\n",
    "        - ensure no duplicate posts after scraping\n",
    "        - process text data to make them suitable for EDA and modelling.\n",
    "        - checking for pseudo duplicates, e.g. megathreads with the same body but different date in title\n",
    "    3. EDA\n",
    "        - examine frequency of features\n",
    "        - examine post length\n",
    "        - examine post engagement via upvotes\n",
    "    4. Modeling\n",
    "        - 2 parametric models\n",
    "            - Logistic Regression, Multnomial Naive Bayes\n",
    "        - 4 non-parametric tree models\n",
    "            - Random Forest, Extra Trees, AdaBoosted trees, GradientBoosted trees\n",
    "        - 2 Vectorizers\n",
    "            - CountVectorizer\n",
    "            - TfidfVectorizer\n",
    "    5. Evaluation of models\n",
    "        - compare models based on Accuracy score and ROC AUC score.\n",
    "        - due to balance of classes, these metrics are very close.\n",
    "        \n",
    "Results show that our Logistic Regression model with TfidfVectorizer produces the best result. It is the least overfit (difference of 5% between train and validation set) and has the highest accuracy (89% after full optimization). Full details of our model testing is discussed in our 3rd notebook.\n",
    "\n",
    "We found that top feature predictors for r/options are more technical in nature and unique to option trading strategies and options in general. Examples include terms like `call`, `put`, `premium`, `credit spread` and such. Top feature predictors for r/stocks show an emphasis on fundamental research on companies or the broader market. Examples include terms like `invest`, `revenue`, `market`, and `company`. \n",
    "\n",
    "Recommendations for future improvement of our classifier include scraping more posts every month to increase the corpus we can train our model on. The exploration of named entity recognition and sentiment analysis is also recommended for users wishing to use our classifier to identify what companies are most frequently discussed on a given subreddit, as well as the valence of the discussion surrounding these companies.\n",
    "\n",
    "A limitation of our current model is that the content of discussion on these subreddits can vary over time, as market trends are constantly evolving. As such, topics that in vogue currently may fade into obscurity 6 months from now, which will throw off the performance of our classifier. Hence, this classifier will need to constantly be retrained at an appropriate time interval in order to maintain its performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up OAuth to use the Reddit API\n",
    "We will use Reddit's recommended method for accessing their API, which require the use of OAuth. If you do not have a Reddit account, you can still access the API by calling the .json version of the page instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import random\n",
    "import time\n",
    "pd.options.display.html.use_mathjax = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# note that CLIENT_ID refers to 'personal use script' and SECRET_TOKEN to 'token'\n",
    "auth = requests.auth.HTTPBasicAuth('<CLIENT_ID', '<SECRET_TOKEN')\n",
    "\n",
    "# here we pass our login method (password), username, and password\n",
    "data = {'grant_type': 'password',\n",
    "        'username': '<USERNAME>',\n",
    "        'password': '<PASSWORD'}\n",
    "\n",
    "# setup our header info, which gives reddit a brief description of our app\n",
    "headers = {'User-Agent': 'classifier proj by u/ltckrompirov'}\n",
    "\n",
    "# send our request for an OAuth token\n",
    "res = requests.post('https://www.reddit.com/api/v1/access_token',\n",
    "                    auth=auth, data=data, headers=headers)\n",
    "\n",
    "# convert response to JSON and pull access_token value\n",
    "TOKEN = res.json()['access_token']\n",
    "\n",
    "# add authorization to our headers dictionary\n",
    "headers = {**headers, **{'Authorization': f\"bearer {TOKEN}\"}}\n",
    "\n",
    "# while the token is valid (~2 hours) we just add headers=headers to our requests, like this\n",
    "requests.get('https://oauth.reddit.com/api/v1/me', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_from_json(url, cycles=10, params={'limit':'100'}):\n",
    "    \n",
    "    # instantiate a dataframe\n",
    "    df = pd.DataFrame()\n",
    "    # this is to tell the api where to start each new cycle from\n",
    "    after = None\n",
    "    url = url\n",
    "    for a in range(cycles):\n",
    "        if after == None:\n",
    "            current_url = url\n",
    "        else:\n",
    "            current_url = url + '?after=' + after\n",
    "        print(current_url)\n",
    "        res = requests.get(current_url, headers=headers, params=params)\n",
    "\n",
    "        if res.status_code != 200:\n",
    "            print('Status error', res.status_code)\n",
    "            break\n",
    "\n",
    "        for post in res.json()['data']['children']:\n",
    "            # append relevant data to dataframe\n",
    "            df = df.append({'subreddit': post['data']['subreddit'],\n",
    "                'title': post['data']['title'],\n",
    "                'selftext': post['data']['selftext'],\n",
    "                'upvote_ratio': post['data']['upvote_ratio'],\n",
    "                'ups': post['data']['ups'],\n",
    "                'downs': post['data']['downs'],\n",
    "                'score': post['data']['score'],\n",
    "                'author': post['data']['author']\n",
    "            }, ignore_index=True)\n",
    "\n",
    "        after = res.json()['data']['after']\n",
    "\n",
    "        # generate a random sleep duration to look more 'natural'\n",
    "        sleep_duration = random.randint(6,12)\n",
    "        print(sleep_duration)\n",
    "        time.sleep(sleep_duration)\n",
    "    # drops all duplicate rows based on the text values of title and selftext\n",
    "    df = df.drop_duplicates(subset = ['title','selftext'])\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from r/options\n",
    "Using our function above makes scraping 1000 posts trivial. Just come back in about 3 minutes.\n",
    "\n",
    "To scrape from the top 1000 posts this month, we have to pass in the parameters appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://oauth.reddit.com/r/options/top\n",
      "10\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lpe4sw\n",
      "10\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lw5mdv\n",
      "12\n",
      "http://oauth.reddit.com/r/options/top?after=t3_m003jm\n",
      "11\n",
      "http://oauth.reddit.com/r/options/top?after=t3_ljftql\n",
      "13\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lioy86\n",
      "11\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lol75v\n",
      "15\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lql8ss\n",
      "17\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lxoxr6\n",
      "10\n",
      "http://oauth.reddit.com/r/options/top?after=t3_lnm8qy\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "df_options = df_from_json('http://oauth.reddit.com/r/options/top', params={'sort':'top','limit':'100','t':'month'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our scraped data\n",
    "df_options_nodup.to_csv('options_topmonth.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scraping from r/stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://oauth.reddit.com/r/stocks/top\n",
      "38\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lj3ec6\n",
      "12\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lkj30k\n",
      "36\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_ly92v2\n",
      "8\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lv5vnf\n",
      "23\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_m06i7b\n",
      "40\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lhspgz\n",
      "30\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lxinux\n",
      "14\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lk4mzl\n",
      "10\n",
      "http://oauth.reddit.com/r/stocks/top?after=t3_lxlp5v\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "df_stocks = df_from_json('http://oauth.reddit.com/r/stocks/top', params={'sort':'top','limit':'100','t':'month'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save our scraped data\n",
    "df_stocks_nodup.to_csv('stocks_topmonth.csv',index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "307.2px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
